\section{Conclusions}
\label{sec:conclusion}

Whole sky spectral radiance distributions are needed for accurate computations in a variety of applications, and yet they are often oversimplified. Real-time capable models are needed to estimate them to within acceptable tolerances. We presented a solution that: (1) took photographs of the entire hemispherical sky and measured the incoming radiance at various points, (2) used those measurements and modern machine learning methods to train regression models, and (3) used those models to predict atmospheric spectral radiance (350-1780 nm) at 1 nm resolution for the entire sky, given a photo of a clear sky and its capture timestamp, in $\mathtt{\sim}$20 s of processing time, making the solution viable for real-time applications. Our whole sky prediction error, for all four holdout test skies, none of which were used for training, was below 7.5\% RMSD, and most of the predicted spectral radiance distributions were in line with libRadtran.

Our results show that image compression, color model, and exposure of clear sky imagery have little to no effect on our method. This implies that our solution is robust and less likely to be affected by  implementation details. We also showed that our models have the ability to generalize across the hemispherical space between measured sky samples, allowing for atmospheric spectral radiance predictions for every point in a sky image.

Our trained models can be used as-is, with similarly exposed and oriented sky photos. And our methods can be reproduced to train models using new datasets. Various sky scanning systems exist which can be employed to provide regional training data. Existing correlated sky imagery and spectral radiance datasets from around the world can (and should) be used with our method. Once normalized, such comprehensive datasets could lead to even more robust models (e.g. more variations of sky turbidity). As mentioned, aerosol data can also be included as training and prediction features, supplied from local building sensors, GOES satellite measurements, and/or triangulated atmospheric measuring station data. Site location coordinates and/or elevation could also be investigated as input features when using multi-site data. More spectral radiance measurements within the circumsolar region would also likely improve accuracy (clear or cloudy sky), as the bulk of the energy is accounted for within that region of the sky. All research in this area could benefit by a scanning pattern that accounts for this.

Although many downstream applications of our research are possible, one immediately viable option is a building monitoring system equipped with all-sky camera that adjusts smart glazing and kinetic facades in response to spectral radiance across the entire non-occluded sky. Such a system would automatically harness (or attenuate) light and heat with more fine-grain control and accuracy than one that operates on a single downwelling measurement, and would be much more affordable and efficient than a live, continuously operating sky scanning system. As mentioned, various procedural processes can be applied to distinguish clear, scattered, and overcast skies, so that pixels and image regions can be passed to appropriate models for spectral radiance prediction. Cloud detection research regularly separates clear from cloudy portions of skies. We hope our research motivates the building performance community to further refine such a system. We also hope that the graphics (rendering) community notices the useful of our sradmap tool. The predicted spectral radiance distributions can and should be used in spectral renderers (the future of rendering) to provide the most accurate natural day-lighting scenes.

Further work will focus on scattered cloudy skies. Scattered skies account for the bulk of our publicly available dataset (63\%), and in general is more complicated to model. More modern, complex machine-learning techniques, such as neural networks, are likely necessary to model the complex non-linear relationships of scattered skies. Simply throwing our entire dataset (clear, scattered, and overcast data) at a neural architecture search (NAS) deep learning neural network infrastructure, we achieved an 83\% R\textsuperscript{2} score, suggesting there is potential for a unified machine learned model. More investigation is needed to find the right network configuration to handle this problem. We also believe that HDR data will have more of an impact on cloudy versus clear skies, because the color gradients are not nearly as uniform. Additional work should include improving our Gaussian weighted color sampling with rectangular (as opposed to square) convolution kernels, to capture the projected solid angle area (ellipse) precisely.

Portions of this work were presented at SPIE Optics and Photonics for Information Processing XII \citep{delrocco_spie}.
